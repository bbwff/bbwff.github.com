I"E8
<p>目录</p>
<ul id="markdown-toc">
  <li><a href="#前言" id="markdown-toc-前言">前言</a></li>
  <li><a href="#背景" id="markdown-toc-背景">背景</a></li>
  <li><a href="#woody的架构" id="markdown-toc-woody的架构">Woody的架构</a></li>
  <li><a href="#spark-sql-job" id="markdown-toc-spark-sql-job">Spark-sql Job</a></li>
  <li><a href="#logical-plan" id="markdown-toc-logical-plan">Logical Plan</a></li>
  <li><a href="#parserrulecontext" id="markdown-toc-parserrulecontext">ParserRuleContext</a></li>
  <li><a href="#语句转换" id="markdown-toc-语句转换">语句转换</a>    <ul>
      <li><a href="#隔离" id="markdown-toc-隔离">隔离</a></li>
      <li><a href="#context" id="markdown-toc-context">Context</a></li>
    </ul>
  </li>
</ul>

<h3 id="前言">前言</h3>

<table>
  <tbody>
    <tr>
      <td>本文讲eBay Spark测试框架-Woody, 其已被发表在公众号eBay技术荟[Hadoop 平台进阶之路</td>
      <td>eBay Spark测试框架–Woody](https://mp.weixin.qq.com/s/PZoGtkPd6RHTEtfwOx2H0w)</td>
    </tr>
  </tbody>
</table>

<p>新版本的Spark拥有更好的性能和稳定性，对于用户来说，如果长期停留在低版本的Spark，不仅会浪费集群资源，还会进一步加大平台管理团队的工作量。如果进行Spark大版本升级，考虑到版本间可能由于计算行为不一致而导致的数据质量问题，用户就要投入大量的精力去对比重要的job在不同版本下的数据质量，加大了版本升级的困难度。</p>

<p>ADI Hadoop team负责管理eBay的Hadoop集群、 Spark的版本升级和bug修复等事务。<strong>为了提升Spark版本升级的效率，本团队开发了Spark测试框架——Woody。</strong>该测试框架会将线上spark-sql job语句转换为和线上job隔离的测试语句，然后调用不同的Spark版本执行测试语句，最终对比版本间数据质量。Woody不仅可以用于Spark版本升级，也可用于job调优以及job pipeline的端到端测试。本文将分享Spark测试框架Woody的架构，实现以及使用场景，希望能对读者有所帮助。</p>

<h3 id="背景">背景</h3>

<p>Hadoop team目前管理两个大Spark分支，Spark-2.1和Spark-2.3，目前的版本开发均基于Spark-2.3，而对于Spark-2.1分支已经不再进行维护，未来会升级到Spark-3.0。</p>

<p>Hadoop team从两年前就着手进行从Spark-2.1 到Spark-2.3的迁移工作，用了将近两年时间完成了迁移。</p>

<p>为什么会用这么长时间呢？</p>

<p>因为大版本之间可能会存在不兼容问题，计算行为可能发生改变，也就是说两个版本间的计算结果可能不一致。</p>

<p>数据质量是至关重要的，特别是对于金融数据，业务团队需要在升级之前进行两个版本间的计算结果对比。</p>

<p>而这需要用户去手动修改线上代码，然后利用两个Spark版本进行双跑，最后再去手动对比两个版本的计算结果。eBay内部的spark-sql任务数不胜数，大版本升级会消耗大量的资源和人力。</p>

<p>Spark-2.1到Spark-2.3 已经耗费了这么长时间，那么将来升级到Spark-3.0想必也是一个浩大的工程。</p>

<p>为了解决这个问题，Hadoop team开发了一个Spark测试框架，命名为Woody。Woody的名字取自一个卡通啄木鸟，希望可以帮助找出不同Spark版本之间或者Spark job中的bug(虫子)。</p>

<p>Woody可以将线上的SQL语句进行转换，然后分别启动两个Spark版本运行转换后的SQL，从而对比两个版本的计算结果，判断两个版本计算结果是否一致，也可以用于比较两个版本的性能。</p>

<p><img src="/public/img/woody/p1.png" alt="" /></p>

<h3 id="woody的架构">Woody的架构</h3>

<p>Woody的架构如图2所示，提供restful api，使用mysql存储数据，支持多个集群。用户可以一次提交一批用于测试的job，Woody用一个workflow封装这批job,由workflow调度器进行调度，每个workflow调度时生成一个对应的jobSetManager,进入job调度器，job调度器会限制同时运行job的数量。</p>

<p><img src="/public/img/woody/p2.png" alt="" /></p>

<p>一个job的生命周期为：</p>

<ol>
  <li>将job语句转换为测试语句</li>
  <li>测试运行前准备工作</li>
  <li>调用Spark版本1运行测试语句</li>
  <li>计算Spark版本1结果的校验信息</li>
  <li>调用Spark版本2运行测试语句</li>
  <li>计算Spark版本2结果的校验信息</li>
  <li>给出数据质量报告</li>
</ol>

<p>关于job语句的转换，Woody为各个集群启动多个长运行的conversion executor，这些conversion executor会向Woody Server进行注册并定期汇报自己的心跳，由Conversion Service Manager来管理。</p>

<p>在job需要运行测试语句阶段，Spark App Submit Service会向相应的集群提交Spark任务, Woody会记录其ApplicationId存入mysql。</p>

<p>Woody使用mysql共享状态数据，支持HA, 是cloud-native的服务。在一台Woody服务关闭时会将其正在运行的workflow标记为游离状态，这些游离状态的workflow会被其他正在运行的Woody服务接管，或者由当前Woody服务重启后重新接管。</p>

<h3 id="spark-sql-job">Spark-sql Job</h3>

<p>首先，介绍一下本文中对source表，working表和target表的定义：</p>

<ul>
  <li>source表是作为输入的表，是被select from的表；</li>
  <li>working表是在job运行中被创建的表，包括(temporary)view；</li>
  <li>target表是被写入数据的表，比如被load数据，或者被insert数据等等。</li>
</ul>

<p>前面提到了用户在测试版本间数据质量的时候，需要手动对两个Spark版本间的计算进行对比，这一操作有以下三个要点:</p>

<ol>
  <li>更改SQL语句，至少需要更改insert数据的表名，避免影响线上job；</li>
  <li>保持source表的数据一致；</li>
  <li>手动检查两个Spark版本的计算结果。</li>
</ol>

<p>Woody需要自动化完成以上三方面的工作。</p>

<p>首先，对于如何去自动地更改线上job语句，请参考以下这组简单的Spark-sql语句：</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">or</span> <span class="k">replace</span> <span class="k">temporary</span> <span class="k">view</span> <span class="n">working_table</span> <span class="k">as</span> <span class="k">select</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="err">…</span><span class="p">,</span> <span class="n">cn</span> <span class="k">from</span> <span class="n">src_table</span><span class="p">;</span>

<span class="k">insert</span> <span class="n">overwrite</span> <span class="n">target_table</span> <span class="k">select</span> <span class="n">working_table</span><span class="p">.</span><span class="n">c1</span> <span class="k">from</span> <span class="n">working_table</span><span class="p">;</span>
</code></pre></div></div>

<p>在这组语句里面，由src_table 作为job的输入，而working_table是在job运行时候生成的，target_table则作为job的输出。</p>

<p>如果要更改线上job语句，不被改变的src_table是不用更改的，作为输出的target_table是必须修改的，临时（temporary）的working表(temporary view)是不用更改的，非临时的working表则必须更改。</p>

<p>那么如何才能找出这些src, working和target表呢？</p>

<h3 id="logical-plan">Logical Plan</h3>

<p>Catalyst是Spark-sql可扩展的SQL优化器。它会将一条SQL语句或者dataframe操作转换为logical plan，然后进行优化，最后转换成可执行的物理计划运行，因篇幅限制，此处不做过多展开。</p>

<p><img src="/public/img/woody/p3.png" alt="" /></p>

<p>如上图所示，Woody会使用其中的unresolved logical plan 来进行分析，来找出其中的source表，working表，target表和一些location等信息。</p>

<p>比如，下面的这句SQL会转换为一个logical plan，可以从叶子节点拿到该语句的source表信息；而对于create table语句，它对应一个CreateTable类型的logical plan，可以从该plan拿到working表信息；相应的insert语句对应于一个InsertIntoTable类型的logical plan，可以从中获得target表等等。</p>

<p><img src="/public/img/woody/p4.png" alt="" /></p>

<p>前文提到可以从一个LogicalPlan的叶子节点拿到其中的source表信息。每个logical plan对应一个抽象语法树，而这颗抽象语法树上面的每个节点也是一个logical plan，所以一条语句的logical plan对应的其实是一个森林，Woody要找到的就是这个森林里面的所有叶子节点。</p>

<p>比如说下面的语句：</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">c1</span><span class="p">,</span> <span class="n">udf</span><span class="p">(</span><span class="n">c2</span><span class="p">)</span> <span class="k">from</span> <span class="p">(</span><span class="k">SELECT</span> <span class="k">current_timestamp</span> <span class="k">as</span> <span class="n">c1</span><span class="p">,</span> <span class="p">(</span><span class="k">SELECT</span> <span class="k">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">from</span> <span class="n">ta</span><span class="p">)</span> <span class="k">as</span> <span class="n">c2</span><span class="p">)</span> <span class="n">tb</span><span class="p">;</span>
</code></pre></div></div>

<p>其中subQuery的Project列表里面的第二列，包含source表信息，Woody需要将这个logical plan对应的森林各个分支都进行遍历，找到所有叶子节点，从中提取出表信息。</p>

<p>当然在一个sql job中是有很多条sql语句的，上一条语句的target表可能是下条语句的source表，前面创建的working表，可能是后面用到的source表，以及语句中会有AlterTableSetLocation等等语句，Woody会把解析过程中的context保存起来，用于串联前后表之间的依赖关系，从而进行语句转换。</p>

<p>不只是表信息，对于location的信息，Woody也要进行提取，用于后面转为测试环境中的location等等。</p>

<h3 id="parserrulecontext">ParserRuleContext</h3>

<p>即使找到了语句中的表信息和location信息，那又要如何利用这些信息对原有的sql语句进行转换呢？单纯的利用logical plan是无法做到的，即使编辑了这个logical plan也无法将其映射为sql文本。</p>

<p>Spark使用antlr4 进行语法分析，每个sql文本初次转换之后会获得一个ParserRuleContext，然后catalyst会基于它生成相应的logical plan。</p>

<p>而ParserRuleContext包含一个inputStream对应原始的SQL文本，而且ParserRuleContext也是一颗树,其各个节点也都有自己的类型，每个节点有字段代表其在该inputStream上面的偏移量。</p>

<p>比如说表名对应TableIdentifierContext，location 对应LocationSpecContext。如果要替换一些表名或者location，只需要找到这些表名所在的ParserRuleContext节点，然后将其对应的文本进行替换，再和前后的文本进行拼接，即可对语句进行转换。</p>

<p>举个例子：</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">ta</span> <span class="n">a</span> <span class="k">JOIN</span> <span class="n">tb</span> <span class="n">b</span> <span class="k">ON</span> <span class="n">a</span><span class="p">.</span><span class="n">id</span><span class="o">=</span><span class="n">b</span><span class="p">.</span><span class="n">id</span><span class="p">;</span>
</code></pre></div></div>

<p>对于这个语句，如果我们要转换它的source表ta和tb,就要先拿到对应的ParserRuleContext，找到表名节点，得到其在ParserRuleContext inputStream之上的偏移量；假设ta对应的TableIdentifierContext偏移量是50-52,而tb对应的偏移量是70-72。</p>

<p>那么替换的结果就是:</p>

<p>originText(0, 49) + replace(ta) + originText(52, 69) + replace(tb) + originText(72, length).</p>

<p>接下来讲Woody的转换规则。</p>

<h3 id="语句转换">语句转换</h3>

<h4 id="隔离">隔离</h4>

<p>首先，转换后的语句必定要与线上环境隔离开来，不能影响线上job和数据。Woody的策略是创建一个数据库专门用于对这个job的测试，然后将该job中要输出的数据全部保存在该数据库下面。</p>

<p>假设这个数据库命名为 WOODY_DB_${UNIQUE_ID}。</p>

<p>对于job中只读的source表，Woody不会去转换这些表的名字。对于job中的输出表，其原有的名字是dbName.tblName, Woody会将其表名转换为WOODY_DB_${UNIQUE_ID}.dbName__tblName，也就是说把数据输出到前面提及到的数据库中,原有的数据库名和表名用两条下划线拼接作为新的表名。</p>

<p>举个例子:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">insert</span> <span class="k">into</span> <span class="n">gdw_tables</span><span class="p">.</span><span class="n">tgt_tbl</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">gdw_tables</span><span class="p">.</span><span class="n">src_tbl</span><span class="err">；</span>
</code></pre></div></div>

<p>会被转换为</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">insert</span> <span class="k">into</span> <span class="n">WOODY_DB_</span><span class="err">{</span><span class="n">UNIQUE_ID</span><span class="err">}</span><span class="p">.</span><span class="n">gdw_tables__tgt_tbl</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">gdw_tables</span><span class="p">.</span><span class="n">src_tbl</span><span class="p">;</span>
</code></pre></div></div>

<h4 id="context">Context</h4>

:ET