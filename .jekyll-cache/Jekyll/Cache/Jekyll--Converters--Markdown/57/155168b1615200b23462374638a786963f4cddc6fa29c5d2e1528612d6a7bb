I" #
<p>目录</p>
<ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#关于spark-sql模块" id="markdown-toc-关于spark-sql模块">关于spark-sql模块</a>    <ul>
      <li><a href="#execution包" id="markdown-toc-execution包">execution包</a></li>
    </ul>
  </li>
  <li><a href="#exchange包" id="markdown-toc-exchange包">exchange包</a>    <ul>
      <li><a href="#exchange--exchangecoordinator" id="markdown-toc-exchange--exchangecoordinator">Exchange &amp; ExchangeCoordinator</a></li>
      <li><a href="#ensurerequirements" id="markdown-toc-ensurerequirements">EnsureRequirements</a></li>
      <li><a href="#join" id="markdown-toc-join">Join</a></li>
      <li><a href="#aggregate" id="markdown-toc-aggregate">Aggregate</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>
<h3 id="background">Background</h3>

<p>关于spark sql 的execution部分源码解析</p>

<h3 id="关于spark-sql模块">关于spark-sql模块</h3>

<p>spark-sql模块下的代码作用有:</p>

<ul>
  <li>sparkSession，SQLContext，DataSet，DataFrameWriter和reader等类</li>
  <li>api 包: python，r的api</li>
  <li>catalog包: catalog以及column，table，function，database的接口类</li>
  <li>expression包: 包括aggregator，UDF， UDAF以及窗口函数</li>
  <li>internal包：包括catalogImpl， HiveSerde，sessionState，sharedState等internal类</li>
  <li>jdbc包： 里面都是方言类 dialect</li>
  <li>sources包: 用于下推至DataSource的filter以及一些DataSource和sql relation相关接口</li>
  <li><strong>streaming</strong>包: DataStreamReader， writer，StreamingQueryException等于streaming有关的类。</li>
  <li>util包: QueryExecutionListener类</li>
  <li><strong>execution</strong>包：本文的重点</li>
</ul>

<h4 id="execution包">execution包</h4>

<p>该包下源码分为:</p>

<ul>
  <li>直接在根目录下
    <ul>
      <li>基本物理操作: coalesceExec, filterExec, projectExec, unionExec, RangeExec, and etc.</li>
      <li>cacheManager：用于cache table.</li>
      <li>一些exec: sort, DataSourceScan</li>
      <li>wholeStageCodegenExec</li>
      <li>SparkPlanner用于生产物理计划的</li>
      <li>limit以及其他操作</li>
    </ul>
  </li>
  <li>aggregate包: 当然是与聚合相关的exec 以及UDAF</li>
  <li>arrow包: arrow也是一种列式存储格式，这个包有他的 writer以及工具类.</li>
  <li>columnar包: 与列状态，访问，类型相关的类</li>
  <li>command包: 一些命令,ddl, analyzeTableCommand, functions, create等命令</li>
  <li>DataSources包: 与DataSource有关，parquet, jdbc, orc等等
    <ul>
      <li>关于DataSource options， writer， 工具类等等</li>
    </ul>
  </li>
  <li>exchange： 里面有broadcastExchangeExec， exchangeCoordinator，shuffleExchangeExec等相关类，后面会重点分析这个,其实在物理计划中转换的重点就是这部分，所以ensureRequirements就在这个包中。</li>
  <li>joins: join相关的， hashJoin, broadcastHashJoin, broadcastNestedLoopJoin, sortMergeJoin,shuffleHashJoinExec.</li>
  <li><strong>streaming</strong>包: 应该是continuous streaming相关的底层实现，应该不是走rdd，是一个真正流式的实现.</li>
  <li>window包: 窗口函数相关exec。</li>
  <li>python包，r包，metric包。</li>
</ul>

<p>下文重点分析 <strong>aggregate</strong>, <strong>exchange</strong>, <strong>joins</strong>包。</p>

<h3 id="exchange包">exchange包</h3>

<h4 id="exchange--exchangecoordinator">Exchange &amp; ExchangeCoordinator</h4>

<p>首先讲一下Exchange，顾名思义就是交换，是为了在多个线程之间进行数据交换，完成并行。</p>

<p>Exchange分为两种，一种是BroadcastExchange另外一种是ShuffleExchange。Broadcast就是将数据发送至driver，然后由driver广播，这适合于数据量较小时候的shuffle。另一种ShuffleExchange就比较常见了，就是多对多的分发。</p>

<p>如果开启了<code class="highlighter-rouge">spark.sql.adaptive.enabled</code>，也就是自适应执行，</p>

<p>那么在使用ShuffleExchange的时候有对应的ExchangeCoordinator；如果没开启ae，那就不需要协调器。</p>

<p>ExchangeCoordinator顾名思义，是Exchange协调器，是一个用于决定怎么在stage之间进行shuffle数据的coordinator。这个协调器用于决定之后的shuffle有多少个partition用来需要fetch shuffle 数据。</p>

<p>一个coordinator有三个参数.</p>

<ul>
  <li>numExchanges</li>
  <li>targetPostShuffleInputSize</li>
  <li>minNumPostShufflePartitions</li>
</ul>

<p>第一个参数是用于表示有多少个ShuffleExchangeExec需要注册到这个coordinator里面。因此，当我们要开始真正执行时，我们需要知道到底有多少个ShuffleExchangeExec。</p>

<p>第二个参数是表示后面shuffle阶段每个partition的输入数据大小，这是用于adaptive-execution的，用于推测后面的shuffle阶段需要多少个partition。可以通过<code class="highlighter-rouge">spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>来配置。</p>

<p>第三个参数是一个可选参数，表示之后shuffle阶段最小的partition数量。如果这个参数被定义，那么之后的shuffle阶段的partition数量不能小于这个值。</p>

<p>Coordinator的流程如下：</p>

<ul>
  <li>在一个<code class="highlighter-rouge">SparkPlan</code>执行之前，对于一个<code class="highlighter-rouge">ShuffleExchangeExec</code>操作，如果它被指定了一个<code class="highlighter-rouge">coorinator</code>，那么它将会注册到这个协调器，这发生在<code class="highlighter-rouge">doPrepare</code>方法里.</li>
  <li>当开始执行<code class="highlighter-rouge">SparkPlan</code>，注册到这个协调器里面的ShuffleExchangeExec将会调用<code class="highlighter-rouge">postShuffleRDD</code>方法来相应的 post-shuffle <code class="highlighter-rouge">ShuffledRowRDD</code>.如果这个协调器已经决定了如何去shuffle data，那么这个Exec会马上获得它对应的<code class="highlighter-rouge">ShuffledRowRDD</code>.</li>
  <li>如果这个coordinator已经决定了如何shuffle data，它会让注册到自己的<code class="highlighter-rouge">ShuffleExchangeExec</code>s来提交pos-shuffle stage。然后基于pre-shuffle阶段partition的统计信息，来决定post-shuffle的partition数量，如果post-shuffle需要，它也会将一些需要的连续的partitions放在一起发送给post-shuffle.</li>
  <li>最后，这个coordinator会为所有注册的<code class="highlighter-rouge">ShuffleExchangeExec</code>创建post-shuffle <code class="highlighter-rouge">ShuffledRowRDD</code>。</li>
</ul>

<p>目前的ae是比较老版本的ae，intel有一个ae项目，相信会在spark-3.0之后会合入，可以了解一下新的ae。</p>

<h4 id="ensurerequirements">EnsureRequirements</h4>

<p>这就是前面说的在SparkPlan 之前的do-prepare，需要为sparkplan的可执行做一些准备工作。</p>

<p>主要分为以下几部分:</p>

<ul>
  <li>确定distribution和Ordering</li>
  <li>确定join 的条件和join keys出现顺序匹配</li>
  <li>创建coordinator
    <ul>
      <li>ae开启</li>
      <li>支持Coordinator
        <ul>
          <li>有ShuffleExchangeExec且是HashPartitionings</li>
          <li>支持Distribution并且child个数大于1</li>
        </ul>
      </li>
      <li>针对ShuffleExchangeExec创建coordinator</li>
      <li>针对一个post-shuffle 对应几个pre-shuffle的创建coordinator，例如join 一个对应多个pre-shuffle，而几个pre-shuffle有不同的分区划分方式</li>
    </ul>
  </li>
</ul>

<h4 id="join">Join</h4>

<p>这里的join是执行阶段的join，不是解析阶段的join。</p>

<p>join包括BroadcastHashJoinExec, ShuffledHashJoinExec以及SortMergeSortMergeJoinExec。</p>

<p>join策略的选择<code class="highlighter-rouge">SparkStrategies</code>类里面，面临join，首先会尝试BroadcastJoin，然后是ShuffledHashJoin最后才是SortMergeHashJoin,只要能满足前面的条件就会优先使用前面的join。</p>

<p>而这些JoinExec是在前面构建物理之前进行构建，在之后真正执行物理计划的时候执行。</p>

<h4 id="aggregate">Aggregate</h4>

<p>Agg和join有些类似，都是需要进行shuffle操作，但不同的是Aggregate可以是一个一元操作，而join是多元操作。</p>

<p>Aggregate操作例如 max, count, min, sum，groupBy， reduce, reduceBy 以及一些UDAF(User Defined Aggregate Function)。而groupBy这些操作可能面临order by.</p>

<h3 id="conclusion">Conclusion</h3>

<p>本文大概讲解了下execution包中各个部分的用途，重点是如何进行Exchange，以及什么是ExchangeCoordinator。对于join和Aggregate未涉及太多。</p>
:ET