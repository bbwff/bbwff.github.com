I"	X
<p>目录</p>
<ul id="markdown-toc">
  <li><a href="#前言" id="markdown-toc-前言">前言</a></li>
  <li><a href="#关于bucket-table" id="markdown-toc-关于bucket-table">关于Bucket Table</a>    <ul>
      <li><a href="#创建bucket-表" id="markdown-toc-创建bucket-表">创建bucket 表</a></li>
      <li><a href="#bucket表参数" id="markdown-toc-bucket表参数">Bucket表参数</a></li>
      <li><a href="#insert-into-bucket-table" id="markdown-toc-insert-into-bucket-table">Insert into bucket table</a></li>
    </ul>
  </li>
  <li><a href="#问题分析" id="markdown-toc-问题分析">问题分析</a>    <ul>
      <li><a href="#double-和decimal" id="markdown-toc-double-和decimal">double 和Decimal</a></li>
    </ul>
  </li>
  <li><a href="#解决方案" id="markdown-toc-解决方案">解决方案</a></li>
  <li><a href="#附录" id="markdown-toc-附录">附录</a>    <ul>
      <li><a href="#验证double-和decimal-类型的bucket-id-是否不同" id="markdown-toc-验证double-和decimal-类型的bucket-id-是否不同">验证double 和Decimal 类型的bucket Id 是否不同</a></li>
    </ul>
  </li>
</ul>
<p>记一次与bucket table相关的小文件问题</p>

<h3 id="前言">前言</h3>

<p>最近遇到了一次跟Spark bucket table相关的小文件问题。</p>

<p>场景如下:</p>

<p>存在一张bucket table, bucket column 是 c1, bucket数量是1000，用户在对这张bucket 表进行insert overwrite的时候，已经将spark.sql.shuffle.partitions 设置成了1000，而且对bucket column那列进行了distribute by 操作，理想情况下，这次overwrite操作将生成1000个小文件，但是出人意料的是，这次操作生成了 1000*1000=100万个小文件!!!</p>

<p>这么多小文件必定需要很多次create 请求和rename请求，因此触发了Hadoop集群报警机制。而且长此以往也会大大增大namenode 的压力。</p>

<h3 id="关于bucket-table">关于Bucket Table</h3>

<p>Spark和Hive中都有bucket table，但是其格式不尽相同。本文不对此进行赘述，关于内容是关于Spark的bucket表。</p>

<p>Bucket表的作用相当于一种数据预处理，如果两个bucket 表的bucket数量相同，且对两个表的bucket key进行join，那个可以避免shuffle 操作，需要数据管理者进行一定的设计。</p>

<h4 id="创建bucket-表">创建bucket 表</h4>

<p>语句格式:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="p">[</span><span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span><span class="p">]</span> <span class="p">[</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span>
  <span class="p">[(</span><span class="n">col_name1</span> <span class="n">col_type1</span> <span class="p">[</span><span class="k">COMMENT</span> <span class="n">col_comment1</span><span class="p">],</span> <span class="p">...)]</span>
  <span class="k">USING</span> <span class="n">data_source</span>
  <span class="p">[</span><span class="k">OPTIONS</span> <span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span> <span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name1</span><span class="p">,</span> <span class="n">col_name2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="n">CLUSTERED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name3</span><span class="p">,</span> <span class="n">col_name4</span><span class="p">,</span> <span class="p">...)</span> <span class="n">SORTED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name1</span> <span class="k">ASC</span><span class="p">,</span> <span class="n">col_name2</span> <span class="k">DESC</span><span class="p">)</span> <span class="k">INTO</span> <span class="n">num_buckets</span> <span class="n">BUCKETS</span><span class="p">]</span>
  <span class="p">[</span><span class="k">LOCATION</span> <span class="n">path</span><span class="p">]</span>
  <span class="p">[</span><span class="k">COMMENT</span> <span class="n">table_comment</span><span class="p">]</span>
  <span class="p">[</span><span class="n">TBLPROPERTIES</span> <span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span> <span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="k">AS</span> <span class="n">select_statement</span><span class="p">]</span>
</code></pre></div></div>

<p>PS: 创建bucket表时候用到的<code class="highlighter-rouge">clustered by (key)</code> 和 <code class="highlighter-rouge">sorted by (key)</code>，和在select 数据时候用的<code class="highlighter-rouge">cluster by key</code> 和<code class="highlighter-rouge">sort by key</code> 很相似，但是用法是不同的。</p>

<p>另外在select 时候有<code class="highlighter-rouge">distribute by</code> 和 <code class="highlighter-rouge">cluster by</code>两种语法，<code class="highlighter-rouge">cluster by key</code> = <code class="highlighter-rouge">distribute by key sort by key</code>. <code class="highlighter-rouge">distribute by key</code> = <code class="highlighter-rouge">hash shuffle by key</code>.</p>

<h4 id="bucket表参数">Bucket表参数</h4>

<p>Spark中有两个bucket表相关参数：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.sql.sources.bucketing.enabled  是否将bucekt表看成是bucekt表
spark.sql.sources.bucketing.maxBuckets 允许的最大bucekt 数量，默认是100000
</code></pre></div></div>

<p>关于第一个参数，作用是否将bucket表看成是bucekt表。</p>

<p>Spark针对bucket表读取的时候，会对每一个bucket分配一个task来读取，因为如果进行bucket join就不能再对这个bucket的数据进行拆分。但是有些时候，我们不是读取这个bucket表进行join，比如是简单的ETL，而此时map阶段会针对每个bucket分配一个mapTask，而如果这个bucket数据量很大，就会很缓慢。而如果此时，我们把spark.sql.sources.bucketing.enabled 设为false，那么就相当于一个普通表，map端可能会针对这个bucket的数据进行split，从而多分配一些task，加快速度。</p>

<h4 id="insert-into-bucket-table">Insert into bucket table</h4>

<p>Insert into bucket table的时候，会加一个针对bucket column的hashPartitioning 函数。因此如果一个task中的数据在insert into这个bucket table的时候，没有提前针对这个bucket column 进行过基于bucket number  的hash(可以将spark.sql.shuffle.partitions 设置为bucket number，然后进行distribute/cluster by),那么每个task 将会生成 bucket number个文件。</p>

<h3 id="问题分析">问题分析</h3>

<p>出现问题的sql语句的执行计划核心部分如下图所示。</p>

<p>可以看到这是对两个子查询进行union，然后我们做了基于bucket column和number的hash(distribute and sort by), 之后insert overwrite 一个bucekt表。</p>

<p>用户期望的结果是会最终产生1000个文件，但是出乎意料的生成了100万个小文件。</p>

<p><img src="/imgs/spark-bucket-small-files/plan.png" alt="" /></p>

<p>通过对语句进行精简，我拿到一个可复现问题的简单测试。</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">sql</span><span class="o">(</span><span class="n">s</span><span class="s">"create table ta (c1 decimal(38, 18), c2 int, p1 int) using parquet partitioned"</span> <span class="o">+</span>
  <span class="s">" by (p1) clustered by (c1) sorted by (c1) into 10 buckets"</span><span class="err">）</span>

<span class="nf">sql</span><span class="o">(</span><span class="s">"set spark.sql.shuffle.partitions=10"</span><span class="o">)</span>    

<span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="nv">Seq</span><span class="o">.</span><span class="py">range</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">1000</span><span class="o">),</span> <span class="mi">1000</span><span class="o">)</span>
  <span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">v</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="nc">Decimal</span><span class="o">(</span><span class="n">v</span><span class="o">),</span> <span class="n">v</span><span class="o">)).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"c1"</span><span class="o">,</span> <span class="s">"c2"</span><span class="o">).</span><span class="py">write</span><span class="o">.</span><span class="py">mode</span><span class="o">(</span><span class="s">"overwrite"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">saveAsTable</span><span class="o">(</span><span class="s">"tb"</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="nv">Seq</span><span class="o">.</span><span class="py">range</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">1000</span><span class="o">),</span> <span class="mi">1000</span><span class="o">)</span>
  <span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">v</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="nv">v</span><span class="o">.</span><span class="py">toDouble</span><span class="o">,</span> <span class="n">v</span><span class="o">)).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"c1"</span><span class="o">,</span> <span class="s">"c2"</span><span class="o">).</span><span class="py">write</span><span class="o">.</span><span class="py">mode</span><span class="o">(</span><span class="s">"overwrite"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">saveAsTable</span><span class="o">(</span><span class="s">"tc"</span><span class="o">)</span>

<span class="nf">sql</span><span class="o">(</span><span class="s">"insert overwrite table ta partition(p1=1) select c1, c2 from tb union all "</span> <span class="o">+</span>
  <span class="s">"select c1, c2 from tc distribute by c1"</span><span class="o">)</span>
</code></pre></div></div>

<p>在这个测试中有三张表，ta是一张bucket表，有三列, c1, c2, p1, 而p1是分区列，c1是bucket列， bucket 数目是10. 我们已经将spark.sql.shuffle.partitions 设置为10.</p>

<p>然后tb 和tc都有两列 c1, c2, 我们将select 两张表的数据，进行union，之后对c1 进行distribute by，之后overwrite 到 ta 中p1=1的分区。</p>

<p>执行之后，查看ta下面p1=1目录，发现有200个小文件(已经很多了)。</p>

<p>查看insert overwrite语句物理执行计划。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">==</span> Physical Plan <span class="o">==</span>
Execute InsertIntoHadoopFsRelationCommand file:/private/var/folders/lw/8qtm67pn1gdb86hj4jcrk_ww39cyt7/T/spark-40631839-0276-414f-aa2f-0721eaab3e26, Map<span class="o">(</span>p1 -&gt; 1<span class="o">)</span>, <span class="nb">false</span>, <span class="o">[</span>p1#255], 10 buckets, bucket columns: <span class="o">[</span>c1], <span class="nb">sort </span>columns: <span class="o">[</span>c1], Parquet, Map<span class="o">(</span>path -&gt; file:/private/var/folders/lw/8qtm67pn1gdb86hj4jcrk_ww39cyt7/T/spark-40631839-0276-414f-aa2f-0721eaab3e26<span class="o">)</span>, Overwrite, CatalogTable<span class="o">(</span>
Database: default
Table: ta
Created Time: Sat Mar 28 10:19:48 PDT 2020
Last Access: UNKNOWN
Created By: Spark 3.1.0-SNAPSHOT
Type: EXTERNAL
Provider: parquet
Num Buckets: 10
Bucket Columns: <span class="o">[</span><span class="sb">`</span>c1<span class="sb">`</span><span class="o">]</span>
Sort Columns: <span class="o">[</span><span class="sb">`</span>c1<span class="sb">`</span><span class="o">]</span>
Location: file:///private/var/folders/lw/8qtm67pn1gdb86hj4jcrk_ww39cyt7/T/spark-40631839-0276-414f-aa2f-0721eaab3e26
Partition Provider: Catalog
Partition Columns: <span class="o">[</span><span class="sb">`</span>p1<span class="sb">`</span><span class="o">]</span>
Schema: root
 |-- c1: decimal<span class="o">(</span>38,18<span class="o">)</span> <span class="o">(</span>nullable <span class="o">=</span> <span class="nb">true</span><span class="o">)</span>
 |-- c2: integer <span class="o">(</span>nullable <span class="o">=</span> <span class="nb">true</span><span class="o">)</span>
 |-- p1: integer <span class="o">(</span>nullable <span class="o">=</span> <span class="nb">true</span><span class="o">)</span>
<span class="o">)</span>, org.apache.spark.sql.execution.datasources.CatalogFileIndex@3faae831, <span class="o">[</span>c1, c2, p1]
+- <span class="k">*</span><span class="o">(</span>3<span class="o">)</span> Project <span class="o">[</span>ansi_cast<span class="o">(</span>c1#250 as decimal<span class="o">(</span>38,18<span class="o">))</span> AS c1#254, c2#247, 1 AS p1#255]
   +- Exchange hashpartitioning<span class="o">(</span>c1#250, 10<span class="o">)</span>, <span class="nb">false</span>, <span class="o">[</span><span class="nb">id</span><span class="o">=</span><span class="c">#155]</span>
      +- Union
         :- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> Project <span class="o">[</span>cast<span class="o">(</span>c1#246 as double<span class="o">)</span> AS c1#250, c2#247]
         :  +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> ColumnarToRow
         :     +- FileScan parquet default.tb[c1#246,c2#247] Batched: <span class="nb">true</span>, DataFilters: <span class="o">[]</span>, Format: Parquet, Location: InMemoryFileIndex[file:/Users/fwang12/todo/apache-spark/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: <span class="o">[]</span>, PushedFilters: <span class="o">[]</span>, ReadSchema: struct&lt;c1:decimal<span class="o">(</span>38,18<span class="o">)</span>,c2:int&gt;
         +- <span class="k">*</span><span class="o">(</span>2<span class="o">)</span> ColumnarToRow
            +- FileScan parquet default.tc[c1#248,c2#249] Batched: <span class="nb">true</span>, DataFilters: <span class="o">[]</span>, Format: Parquet, Location: InMemoryFileIndex[file:/Users/fwang12/todo/apache-spark/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: <span class="o">[]</span>, PushedFilters: <span class="o">[]</span>, ReadSchema: struct&lt;c1:double,c2:int&gt;

</code></pre></div></div>

<p>原来虽然表tb 和tc中都有名为c1的列，但是一个是Decimal类型，一个是double类型。在做union的时候，取 double 和Decimal的公共类型double，因此对表tb中的c1 进行了cast(c1 as double)，但是在insert overwrite bucket table的时候，由于bucket column c1的类型是Decimal，所以我们可以看到在进行<code class="highlighter-rouge">Exchange hashpartitioning</code> 之后进行了<code class="highlighter-rouge">Project [ansi_cast(c1#250 as decimal(38,18)) AS c1#254, c2#247, 1 AS p1#255]</code>  操作，又把double转成了Decimal.</p>

<p>因此，我们前面hash 后的数据是基于double类型的hash，而bucket column类型是Decimal类型。</p>

<p>查看FileFormatWriter中的write方法，其逻辑为，如果currentPartitionValue或者currentBucketId与next 不同。</p>

<p>那么会创建一个新的文件。</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nf">if</span> <span class="o">(</span><span class="n">currentPartionValues</span> <span class="o">!=</span> <span class="n">nextPartitionValues</span> <span class="o">||</span> <span class="n">currentBucketId</span> <span class="o">!=</span> <span class="n">nextBucketId</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// See a new partition or bucket - write to a new partition dir (or a new bucket file).</span>
      <span class="nf">if</span> <span class="o">(</span><span class="n">isPartitioned</span> <span class="o">&amp;&amp;</span> <span class="n">currentPartionValues</span> <span class="o">!=</span> <span class="n">nextPartitionValues</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">currentPartionValues</span> <span class="k">=</span> <span class="nc">Some</span><span class="o">(</span><span class="nv">nextPartitionValues</span><span class="o">.</span><span class="py">get</span><span class="o">.</span><span class="py">copy</span><span class="o">())</span>
        <span class="nv">statsTrackers</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">newPartition</span><span class="o">(</span><span class="nv">currentPartionValues</span><span class="o">.</span><span class="py">get</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="nf">if</span> <span class="o">(</span><span class="n">isBucketed</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">currentBucketId</span> <span class="k">=</span> <span class="n">nextBucketId</span>
        <span class="nv">statsTrackers</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">newBucket</span><span class="o">(</span><span class="nv">currentBucketId</span><span class="o">.</span><span class="py">get</span><span class="o">))</span>
      <span class="o">}</span>

      <span class="n">fileCounter</span> <span class="k">=</span> <span class="mi">0</span>
      <span class="nf">newOutputWriter</span><span class="o">(</span><span class="n">currentPartionValues</span><span class="o">,</span> <span class="n">currentBucketId</span><span class="o">)</span>
    <span class="o">}</span>
</code></pre></div></div>

<p>而double类型和Decimal类型getBucketId的算法是不同的，附录中提供了验证方法。</p>

<p>因此，hash后的double类型数据，在转为Decimal之后，虽然其partitionValue获取是一致的，但是bucketId获取方法存在差异，因此一个task依然生成了十个bucket文件，造成了小文件的爆炸。</p>

<h4 id="double-和decimal">double 和Decimal</h4>

<p>除此之外，此处在简单介绍下double 和Decimal。</p>

<p>double是一种采用科学计数法进行表示的模糊类型，其增大了表示范围，但是却丢失了表示精度。详细可参考前面写的一篇文章<a href="/essay/2020/01/10/重新了解数字类型">重新了解数字类型</a>。</p>

<p>而且前面提到的用户union 两个子查询，子查询有同名的列，一列是double， 一列是Decimal，而且以这两列进行Join key 进行join。</p>

<p>double是一个模糊类型， Decimal是一个精确类型，在spark中比较double 和Decimal的时候会将Decimal转化为double(强制类型转化) 然后与double进行比较，而这种比较是不精确的，容易造成输出爆炸。</p>

<p>因此慎用double类型作为join key。</p>

<p>例如下面的两个double 是不同的数字，比较相等却是true。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scala&gt; 112345678901234568d <span class="o">==</span> 112345678901234560d
res0: Boolean <span class="o">=</span> <span class="nb">true</span>
</code></pre></div></div>

<h3 id="解决方案">解决方案</h3>

<p>解决问题，可以从三方面考虑。</p>

<ol>
  <li>绕过这个小文件问题</li>
  <li>通过重建表从根本解决问题</li>
  <li>平台能不能</li>
</ol>

<h3 id="附录">附录</h3>

<h4 id="验证double-和decimal-类型的bucket-id-是否不同">验证double 和Decimal 类型的bucket Id 是否不同</h4>

<p>此处使用json格式是因为其可以直接查看数据。</p>

<p>在跑完之后去查看表下面的文件，文件名格式:</p>

<p>part-{partitionId}-bdc018dc-38ad-4233-91ff-16f777ad812e_{bucketId}.c000.json</p>

<p>我们只需对比两张表同样bucketId 文件下面的内容不同即可得出结论。</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">sql</span><span class="o">(</span><span class="s">"create table ta(c1 double, c2 int) using json clustered by(c1) into 10 buckets"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"create table tb(c1 decimal(38, 18), c2 int) using json clustered by (c1) into 10 buckets"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"create table tc(c1 double, c2 int) using json"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"create table td(c1 decimal(38, 18), c2 int) using json"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"insert into tc select * from values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9), (10, 10)"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"insert into td select * from values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(10,10)"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"insert overwrite table ta select * from tc"</span><span class="o">)</span>
<span class="nf">sql</span><span class="o">(</span><span class="s">"insert overwrite table tb select * from td"</span><span class="o">)</span>
</code></pre></div></div>

:ET